# -*- coding: utf-8 -*-
"""ECMI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VJ815x3ICtpAepDtFPjBtv5ANPybcaYR
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

data = pd.read_csv("Car insurance data 2022.csv")

df = pd.DataFrame(data.values)
claims = np.concatenate((df[12].values, df[13].values, df[14].values, df[15].values),axis=0)
claims = claims[~pd.isnull(claims)]

df1=df[(df['Income']=="Low")]["Total"]
df2=df[(df['Income']=="Middle")]["Total"]
df3=df[(df['Income']=="High")]["Total"]
print(np.mean(df1))
print(np.mean(df2))
print(np.mean(df3))
kruskal(df1,df2,df3)

data_crosstab = pd.crosstab(data['Education'],
                            data['Nuber of claims'],
                               margins = True,
                               normalize=False)
print(data_crosstab)

chi2, p, dof, ex = stats.chi2_contingency(data_crosstab)

print(f'Chi_square value {chi2}\n\np value {p}\n\ndegrees of freedom {dof}')

import seaborn as sns

# plotting a vertical box plot with Academy and Age
sns.boxplot( x = 'Driving Experience',y = 'Total claim amount (EUR)', hue = 'Sex', data = data_avg_claims , palette="Set3")
# display
plt.title('Data only with claims')
plt.show()

df = pd.DataFrame(data.values)
claims = np.concatenate((df[12].values, df[13].values, df[14].values, df[15].values),axis=0)
claims = claims[~pd.isnull(claims)]

plt.hist(data["Nuber of claims"])

from collections import Counter
Counter(data["Nuber of claims"])

# per user not per claim
plt.hist(data["Total claim amount (EUR)"]/data["Nuber of claims"], bins=100)

"""POISSON"""

from scipy.optimize import curve_fit
from scipy.stats import poisson, geom, nbinom
# the bins have to be kept as a positive integer because poisson is a positive integer distribution
bins = np.arange(4) - 0.5
entries, bin_edges, patches = plt.hist(data["Nuber of claims"], bins=bins, density=True, label='Data')

# calculate bin centers
middles_bins = (bin_edges[1:] + bin_edges[:-1]) * 0.5

def fit_function(k, lamb):
    # The parameter lamb will be used as the fit parameter
    return poisson.pmf(k, lamb)


p01 = np.nanmean(data["Nuber of claims"])/np.nanstd(data["Nuber of claims"])**2
p02 = np.nanmean(data["Nuber of claims"])**2 / ((np.nanstd(data["Nuber of claims"])**2)-np.nanmean(data["Nuber of claims"]))
print(p01, p02)
# fit with curve_fit
parameters, cov_matrix = curve_fit(fit_function, middles_bins, entries, maxfev=100000)

# plot poisson-deviation with fitted parameter
x_plot = np.arange(0, 5)

plt.plot(
    x_plot,
    fit_function(x_plot, *parameters),
    marker='D', linestyle='-',
    color='red',
    label='Fit result',
)
plt.legend()
plt.title("Poisson")
plt.show()

print(parameters)
data3 = np.random.poisson(parameters[0], size=100000)
print(Counter(data3))
print(Counter(data["Nuber of claims"]))

"""GEOMETRIC DISTRIBUTION"""

import numpy as np
from scipy.stats import geom
from scipy.optimize import minimize

# Define the negative log-likelihood function
def neg_log_likelihood(params, data):
    p = params[0]
    return -np.sum(geom.logpmf(data, p))


# Fit the geometric distribution using maximum likelihood estimation
initial_params = [0.5]  # Initial guess for the parameter
result = minimize(neg_log_likelihood, initial_params, args=(data["Nuber of claims"],))
p_hat = result.x[0]

# Print the estimated parameter
print("Estimated parameter (p):", p_hat)

# Plot the fitted distribution and the data
x = np.arange(1, 5 + 1)  # Values for x-axis
y = geom.pmf(x, 0.9567)  # Fitted distribution

plt.figure(figsize=(8, 6))
plt.hist(data["Nuber of claims"], bins=np.arange(1, 5 + 2), density=True, alpha=0.6, label='Data')
plt.plot(x, y, 'ro-', label='Fitted Distribution')
plt.xlabel('x')
plt.ylabel('Probability')
plt.title('Geometric Distribution Fit')
plt.legend()
plt.grid(True)
plt.show()

data3 = np.random.geometric(p=0.9567, size=100000)
print(Counter(data3))
print(Counter(data["Nuber of claims"]))

"""NEGATIVE BINOMIAL DISTRIBUTION"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import nbinom
from scipy.optimize import minimize

data2 = data["Nuber of claims"]+1
Counter(data2)

# Define the negative log-likelihood function
def neg_log_likelihood(params, data):
    r, p = params
    return -np.sum(nbinom.logpmf(data, r, p))


# Fit the negative binomial distribution using maximum likelihood estimation
initial_params = [1, 0.5]  # Initial guess for the parameters
result = minimize(neg_log_likelihood, initial_params, args=(data2,))
r_hat, p_hat = result.x

# Print the estimated parameters
print("Estimated parameters (r, p):", r_hat, p_hat)

p01 = np.nanmean(data["Nuber of claims"])/np.nanstd(data["Nuber of claims"])**2
p02 = np.nanmean(data["Nuber of claims"])**2 / ((np.nanstd(data["Nuber of claims"])**2)-np.nanmean(data["Nuber of claims"]))
print(p01, p02)

# Plot the fitted distribution and the data
x = np.arange(0, np.max(data2) + 1)  # Values for x-axis
y = nbinom.pmf(x, p02, p01)  # Fitted distribution

plt.figure(figsize=(8, 6))
plt.hist(data["Nuber of claims"], bins=np.arange(0, np.max(data2) + 2), density=True, alpha=0.6, label='Data')
plt.plot(x, y, 'ro-', label='Fitted Distribution')
plt.xlabel('x')
plt.ylabel('Probability')
plt.title('Negative Binomial Distribution Fit')
plt.legend()
plt.grid(True)
plt.show()

data3 = np.random.negative_binomial(n=p02, p=p01, size=100000)
print(Counter(data3))
print(Counter(data["Nuber of claims"]))

"""to calculate the bayesian information criterion do like this:"""

# Calculate the Bayesian Information Criterion (BIC)
def calculate_bic(nll, num_params, num_data_points):
    bic = np.log(num_data_points) * num_params - 2 * nll
    return bic

def neg_log_likelihood(params, data):
    r, p = params
    print(r,p)
    return -np.nansum(nbinom.logpmf(data, r, p))

data2 = data["Nuber of claims"]+1
Counter(data2)

# Calculate the negative log-likelihood
parameters = [p02,p01]
print(parameters)
nll = neg_log_likelihood(parameters, data2)
print(nll)

# Calculate the number of parameters and data points
num_params = 2
num_data_points = len(data2)

# Calculate the Bayesian Information Criterion (BIC)
bic = calculate_bic(nll, num_params, num_data_points)
print(bic)

stats.probplot(claims.tolist(),dist="lognorm", sparams=res.params,plot=plt)
plt.show()



# Perform the Kolmogorov-Smirnov test
kstest_result = kstest(claims, 'gamma', args=(shape, loc, scale))

# Set significance level (alpha)
alpha = 0.05

# Print the test result
if kstest_result.pvalue > alpha:
    print("The claim size data follows a Gamma distribution.")
else:
    print("The claim size data does not follow a Gamma distribution.")
print("Kolmogorov-Smirnov test statistic:", kstest_result.statistic)
print("Kolmogorov-Smirnov test p-value:", kstest_result.pvalue)

df['Mileage (km)'] = pd.cut(df['Mileage (km)'], bins=3, labels=['1','2','3'], include_lowest=True)

from scipy.optimize import curve_fit
from scipy.stats import poisson, geom, nbinom
# the bins have to be kept as a positive integer because poisson is a positive integer distribution
bins = np.arange(4) - 0.5
entries, bin_edges, patches = plt.hist(data["Nuber of claims"], bins=bins, density=True, label='Data')

# calculate bin centers
middles_bins = (bin_edges[1:] + bin_edges[:-1]) * 0.5

def fit_function(k, lamb):
    # The parameter lamb will be used as the fit parameter
    return poisson.pmf(k, lamb)


p01 = np.nanmean(data["Nuber of claims"])/np.nanstd(data["Nuber of claims"])**2
p02 = np.nanmean(data["Nuber of claims"])**2 / ((np.nanstd(data["Nuber of claims"])**2)-np.nanmean(data["Nuber of claims"]))
print(p01, p02)
# fit with curve_fit
parameters, cov_matrix = curve_fit(fit_function, middles_bins, entries, maxfev=100000)

# plot poisson-deviation with fitted parameter
x_plot = np.arange(0, 5)

plt.plot(
    x_plot,
    fit_function(x_plot, *parameters),
    marker='D', linestyle='-',
    color='red',
    label='Fit result',
)
plt.legend()
plt.title("Poisson")
plt.show()

print(parameters)
data3 = np.random.poisson(parameters[0], size=100000)
print(Counter(data3))
print(Counter(data["Nuber of claims"]))

# Calculate the Bayesian Information Criterion (BIC)
def calculate_bic(nll, num_params, num_data_points):
    bic = np.log(num_data_points) * num_params - 2 * nll
    return bic

def neg_log_likelihood(params, data):
    return -np.nansum(poisson.logpmf(data, params))

data2 = data["Nuber of claims"]+1
Counter(data2)

# Calculate the negative log-likelihood
print(parameters)
nll = neg_log_likelihood(parameters[0], data2)
print(nll)

# Calculate the number of parameters and data points
num_params = 2
num_data_points = len(data2)

# Calculate the Bayesian Information Criterion (BIC)
bic = calculate_bic(nll, num_params, num_data_points)
print(bic)